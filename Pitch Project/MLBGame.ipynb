{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Next Pitch Type\n",
    "Using the [**mlbgame**](http://panz.io/mlbgame/) library, we can build a training set of pitch sequences. Then, we will use that to train an RNN to predict the next pitch given the previous pitches in an at bat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Pitches\n",
    "We need to get the actual pitch sequences out of the MLB data.  So lets start by getting just the pitches from one game.  We get the events, then for each inning we can look at each at bat in the top and bottom of the inning.  \n",
    "\n",
    "Pitch mappings:\n",
    "\n",
    "* A - Changeup (CH)\n",
    "* B - Curveball (CU)\n",
    "* C - Cutter (FC)\n",
    "* D - Eephus (EP)\n",
    "* E - Forkball (FO)\n",
    "* F - Four-Seam Fastball (FF) - Seems to be a discrepency here.  Official site says FA. Other tutorials disagree.  Data uses FF.  Dont see any FA's.\n",
    "* G - Knuckleball (KN)\n",
    "* H - Knuckle-curve (KC)\n",
    "* I - Screwball (SC)\n",
    "* J - Sinker (SI)\n",
    "* K - Slider (SL)\n",
    "* L - Splitter (FS)\n",
    "* M - Two-Seam Fastball (FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mlbgame\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "pitch_dict = {\"CH\":\"A\", \n",
    "              \"CU\":\"B\", \n",
    "              \"FC\":\"C\", \n",
    "              \"EP\":\"D\", \n",
    "              \"FO\":\"E\", \n",
    "              \"FF\":\"F\", \n",
    "              \"KN\":\"G\", \n",
    "              \"KC\":\"H\", \n",
    "              \"SC\":\"I\", \n",
    "              \"SI\":\"J\", \n",
    "              \"SL\":\"K\", \n",
    "              \"FS\":\"L\", \n",
    "              \"FT\":\"M\",\n",
    "              \"PO\":\"\", # Not sure what these three are\n",
    "              \"IN\":\"\", # Weird\n",
    "              \"UN\":\"\", # Weird\n",
    "              \"\":\"\"}\n",
    "\n",
    "def get_pitch_seqs_from_game( game_id ):\n",
    "    pitches = []\n",
    "    events  = mlbgame.game_events(game_id)\n",
    "    for i in events:\n",
    "        inning = events[i]\n",
    "        pitch_str = \"\"\n",
    "        for ab in inning['top']+inning['bottom']:\n",
    "            for pitch in ab.pitches:\n",
    "                pitch_str += pitch_dict[ pitch.pitch_type ]\n",
    "        pitches.append(pitch_str)\n",
    "    return pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the pitch sequences to strings of single letter identifiers.  Note - I did this initially because I thought it would help putting the data into a network, but I dont think it was necessary.  I ended up making 1-hot vectors, so this is probably an unneeded extra step.\n",
    "\n",
    "Next is a method to abstract getting a set of pitch seqences from a list of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pitch_data_from_game_list( games ):\n",
    "    pitch_data = []\n",
    "    for game in games:\n",
    "        try:\n",
    "            pitch_data += get_pitch_seqs_from_game( game.game_id )\n",
    "        except:\n",
    "            print(\"error with game: \"+game.game_id)\n",
    "    return pitch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets see if we can get a few years of data.  Let's abstract away a method for loading data by year.  This is crude, but as long as we give it well behaved year identifiers we should be ok.  Make sure the ```pitch_data``` directory exists or you'll wait a long while to see an annoying error with open. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PITCH_DATA_DIR = \"pitch_data/\"\n",
    "def load_pitch_by_year( year ):\n",
    "    fname = PITCH_DATA_DIR+\"pitches_\"+str(year)+\".p\"\n",
    "    if os.path.isfile(fname):\n",
    "        pitch_data = pickle.load( open( fname, \"rb\") )\n",
    "    else:\n",
    "        year = mlbgame.games( year )\n",
    "        games = mlbgame.combine_games( year )\n",
    "        pitch_data = get_pitch_data_from_game_list( games )\n",
    "        pitch_data_clean = [s for s in pitch_data if s != ''] # Removes the empty atbats.  \n",
    "        pitch_data = pitch_data_clean\n",
    "        pickle.dump( pitch_data, open( fname, \"wb\" ) )\n",
    "    return pitch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with game: 2016_03_22_anamlb_slcaaa_1\n",
      "error with game: 2016_03_22_tbamlb_cubint_1\n",
      "error with game: 2016_03_31_phimlb_pfsmin_1\n",
      "error with game: 2016_03_31_sdnmlb_elpaaa_1\n",
      "error with game: 2016_04_02_detmlb_atlmlb_1\n",
      "error with game: 2016_04_02_milmlb_blxaax_1\n",
      "error with game: 2016_04_04_bosmlb_clemlb_1\n",
      "error with game: 2016_04_04_houmlb_nyamlb_1\n",
      "error with game: 2016_04_09_miamlb_wasmlb_1\n",
      "error with game: 2016_04_10_nyamlb_detmlb_1\n",
      "error with game: 2016_04_17_balmlb_texmlb_1\n",
      "error with game: 2016_04_27_milmlb_chnmlb_1\n",
      "error with game: 2016_04_28_pitmlb_colmlb_1\n",
      "error with game: 2016_04_30_atlmlb_chnmlb_1\n",
      "error with game: 2016_05_16_bosmlb_kcamlb_1\n",
      "error with game: 2016_05_26_chamlb_kcamlb_1\n",
      "error with game: 2016_06_08_clemlb_seamlb_1\n",
      "error with game: 2016_09_25_atlmlb_miamlb_1\n",
      "error with game: 2016_10_03_clemlb_detmlb_1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pitch_data/pitches_2016.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6bc5fd35091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myears_wanted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpitches_by_year\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pitch_by_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1fc8a2f955df>\u001b[0m in \u001b[0;36mload_pitch_by_year\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpitch_data_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpitch_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Removes the empty atbats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpitch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpitch_data_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpitch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpitch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pitch_data/pitches_2016.p'"
     ]
    }
   ],
   "source": [
    "# If the list of years contains a new year, this will take a WHILE.\n",
    "years_wanted = [2016, 2015, 2014]\n",
    "pitches_by_year = {}\n",
    "\n",
    "for y in years_wanted:\n",
    "    pitches_by_year[str(y)] = load_pitch_by_year(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2016 count: 23561\n",
      "year: 2015 count: 23085\n",
      "year: 2014 count: 22781\n",
      "total: 69427\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for y in years_wanted:\n",
    "    total += len(pitches_by_year[str(y)])\n",
    "    print( \"year: \"+str(y)+\" count: \"+str(len(pitches_by_year[str(y)])) )\n",
    "print(\"total: \"+str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet.  Thats about 70k sequences to train off of.  \n",
    "To train, I think the pitch types need to be converted to sequences of 1-Hot input vectors.  There are 13 possible pitch types, so each pitch will be a vector of size 13.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBKKAFKBFKFBMFKMMFFFKFKMMMKKMFKMMMAFKFMFAKMMMMFMBFBFFFFFKFFACCFBABCCJBAAJAJCJCAAFCACJKKKKFF len: 91\n",
      "MBFKABFKFKMMMBMMMFFKFAMMMMMABMFCFFCFFCFKFFKCFCFFFFHFFFFFHHFAFFFFFHAFFFAFHFFFFMAFMCCFFCMMBF len: 90\n",
      "FKFFFKFFFFKMMBKKJAMMBMMBMJJBMMMMJMBJMFFFBFAFFFBFFBAFFFAFFFBFAFBBFFFBFFFMKMMKKMKMMKKKKKKK len: 88\n"
     ]
    }
   ],
   "source": [
    "# I'll need to know the length of the longest sequence so I can size the time-step of the RNN.\n",
    "for y in years_wanted:\n",
    "    longest = max( pitches_by_year[str(y)], key=len )\n",
    "    print(longest + \" len: \" + str(len(longest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "First, I'll need to turn the data set into a huge tensor of the shape [ NUM_SEQ, MAX_SEQ_LEN, NUM_PITCH_TYPES ].  Or atleast I think thats the shape I want.  \n",
    "\n",
    "TODO - Fix the shapes.  :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "VECTOR_SIZE = 13 # Number of pitch types\n",
    "\n",
    "pitch_char_dict = { \"A\":0,\n",
    "                    \"B\":1,\n",
    "                    \"C\":2,\n",
    "                    \"D\":3,\n",
    "                    \"E\":4,\n",
    "                    \"F\":5,\n",
    "                    \"G\":6,\n",
    "                    \"H\":7,\n",
    "                    \"I\":8,\n",
    "                    \"J\":9,\n",
    "                    \"K\":10,\n",
    "                    \"L\":11,\n",
    "                    \"M\":12}\n",
    "\n",
    "# Create 1-Hot vectors from data set\n",
    "def create_one_hot_series( seq ):\n",
    "    vectors = []\n",
    "    for char in seq:\n",
    "        v = np.zeros( VECTOR_SIZE )\n",
    "        v[ pitch_char_dict[char] ] = 1.0\n",
    "        vectors.append(v)\n",
    "    return vectors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'years_wanted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f73b91b618f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the actual data set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfull_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myears_wanted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfull_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpitches_by_year\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'years_wanted' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the actual data set.\n",
    "full_list = []\n",
    "for y in years_wanted:\n",
    "    full_list += pitches_by_year[str(y)]\n",
    "\n",
    "X_full = []\n",
    "for seq in full_list:\n",
    "    X_full.append( create_one_hot_series( seq ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578936 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print( str(sys.getsizeof(X_full)) + \" bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pitch Prediction Network\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Construction Phase ###############\n",
    "NUM_INPUTS  = 13    # Size of the input vector (the number of possible pitch types)\n",
    "NUM_OUTPUTS = 13    # Want a pitch type out, so same size as input.\n",
    "NUM_NEURONS = 5     # Just a placeholder.  Its what the book uses for the sequence example \n",
    "NUM_STEPS   = 91    # Size of largest input sequence - So all training can fit.\n",
    "\n",
    "## RNN Graph\n",
    "X = tf.placeholder( tf.float32, [None, NUM_STEPS, NUM_INPUTS] ) \n",
    "y = tf.placeholder( tf.float32, [None, NUM_STEPS, NUM_INPUTS] ) # X, but shifted to the left 1 element.\n",
    "seq_len = tf.placeholder( tf.int32, [None] ) # 1D Tensor to hold the length of each sequence in a batch\n",
    "\n",
    "basic_cell   = tf.contrib.rnn.BasicRNNCell( num_units=NUM_NEURONS )\n",
    "wrapped_cell = tf.contrib.rnn.OutputProjectionWrapper( basic_cell, output_size=NUM_OUTPUTS )\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn( wrapped_cell, X, dtype=tf.float32, sequence_length=seq_len ) \n",
    "\n",
    "## Cost Function for Training\n",
    "LEARNING_RATE = 0.001\n",
    "loss        = tf.reduce_mean( tf.square( outputs-y ) )\n",
    "optimizer   = tf.train.AdamOptimizer( learning_rate=LEARNING_RATE )\n",
    "training_op = optimizer.minimize( loss )\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_batch( X_full, batch_number, batch_size ):\n",
    "    start = batch_number*batch_size\n",
    "    end   = start+batch_size\n",
    "    if end > len(X_full):\n",
    "        end = len(X_full)-1\n",
    "    X_batch = X_full[start:end]\n",
    "    y_batch = [ s[1:] for s in X_batch ]  # Shift the sequence to get a y\n",
    "    seq_lens = [ len(s) for s in X_batch ]\n",
    "    return np.array(X_batch), np.array(y_batch), np.array(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-cd8ac3cb3236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Workspace/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_Workspace/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/Documents/ML_Workspace/env/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "### Training Phase\n",
    "BATCH_SIZE = 100\n",
    "NUM_ITERATIONS = int(len(X_full) / BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        X_batch, y_batch, seq_lengths = get_training_batch( X_full, iteration, BATCH_SIZE )\n",
    "        sess.run( training_op, feed_dict={X: X_batch, y: y_batch, seq_len: seq_lengths})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
